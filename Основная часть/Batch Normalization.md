Batch Normalization - это метод нормализации активаций внутри слоев сети, который помогает уменьшить внутреннее смещение (internal covariate shift) и ускорить сходимость обучения. Он также может сделать сеть более робастной к изменениям входных данных и помочь предотвратить взрывные или затухающие градиенты.

Была взята выборка 
Чтобы увидеть, как работает пакетная нормализация, мы построим нейронную сеть с использованием Pytorch и протестируем ее на наборе данных MNIST.

![](https://machinelearningmastery.ru/img/0-755685-742732.png)

Чтобы лучше понять, как пакетная нормализация помогает в более быстрой конвергенции сети, мы рассмотрим распределение значений по нескольким скрытым слоям в сети на этапе обучения.

Для согласованности мы построим график вывода второго линейного слоя из двух сетей и сравним распределение выходных сигналов этого уровня по сетям. Результаты выглядят так:

![](https://machinelearningmastery.ru/img/0-531163-109143.png)

Первый ряд указывает на первую эпоху и второй ряд для второй эпохи

Из графиков можно сделать вывод, что распределение значений без пакетной нормализации значительно изменилось между итерациями входных данных в каждой эпохе.**это означает, что последующие слои в сети без пакетной нормализации видят различное распределение входных данных**, Но изменение в распределении значений для модели с пакетной нормализацией представляется незначительным.

Кроме того, мы можем видеть, что потеря сети с нормализацией пакета уменьшается намного быстрее, чем нормальная сеть из-за ковариационного сдвига, т. Е. Смещения скрытых значений для каждой партии ввода. Это помогает быстрее сходиться в сети и сокращает время обучения.

![](https://machinelearningmastery.ru/img/0-899164-35415.png)