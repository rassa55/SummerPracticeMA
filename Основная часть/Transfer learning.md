Использование предобученных моделей (Transfer learning): Использование предобученных моделей, обученных на больших наборах данных, и их дообучение на своих данных может помочь ускорить процесс обучения и улучшить обобщающую способность сети.

Нейросети, обученные под одну задачу, можно научить решать другие, похожие. Отсюда и название метода трансферного обучения — мы будто «переносим» (от англ. transfer — передача) предыдущий опыт решения проблемы на новую, еще не решенную. Например, зная английский, вам будет легче выучить немецкий, поскольку оба этих языка — западно-германские, а значит, в них присутствуют схожие структуры и закономерности. Так же и нейросеть, обученная решению одной конкретной задачи, может использовать приобретённые знания при решении новой задачи.

Однако зачем обучать модель с «чистого листа», если существует модель, которая уже обучена генерации текстов на русском языке? Так как модель уже «говорит» на русском, обучить её стилю Достоевского намного проще, чем обучать модель, которая даже «не знает» языка. Другими словами достаточно взять готовую русскоязычную версию [GPT3](https://sysblok.ru/glossary/chto-takoe-nejroset-gpt-3-i-zachem-ona-nuzhna/), найти в интернете оцифрованные версии произведений Федора Михайловича (например, [здесь](http://az.lib.ru/d/dostoewskij_f_m/)) и дообучить модель на новых текстах. Эта процедура дообучения и является примером **Transfer Learning (трансферного обучения).** Теперь подробней рассмотрим, как происходит этот «трансфер» знаний.

## Как работает Transfer Learning?

Нейросеть представляет собой последовательность слоев, где каждый слой — математическое преобразование входных данных. Такую последовательность принято называть архитектурой. Как здания различаются по количеству этажей, их типу — например, есть ли коммерческие этажи или все жилые, так и разным архитектурам присущ свой объем слоев, свои типы и их упорядоченность. 

![](https://lh5.googleusercontent.com/a5lnPI_apwItGrD410dgw40CmXUkZnjCrwnvfmXZQ_M9hdY_QGH_NIhho-k4Sr19K_clPQeem0RH5yQhUb7fN7Siv95qU0vy1JgDyF2P95xeieIts7-h37DzMeD3QbuQs8duQXeVLwzxL8IsAMKHKbI)

_Так может выглядеть структура нейросети: input layer — входной слой, то, что мы подаем модели на «обработку». Hidden layer 1 и 2 — два скрытых слоя. Они нужны для преобразования входных данных в более сложные представления, которые затем используются для построения выходных данных. Output layer — выходной слой, то есть то, что нам выдает модель в качестве результата.

Итак, представим, что мы хотим научить модель определять по фотографии больна ли клубника. Собрать огромный набор данных по такой узкой задаче было бы дорого и достаточно проблематично, ведь нужны были бы гигабайты специфических картинок с этой ягодой. Вместо этого можно взять уже предобученную на фотографиях ягод модель компьютерного зрения и «настроить» ее на относительно небольшом наборе данных под нашу проблему. То есть нам надо перенести уже полученные, обобщенные знания о ягодах на классификацию клубники. 

В машинном обучении мы можем выбирать, какие слои нейросети нам обучать, а какие нет. Если мы не хотим обновлять, то есть обучать, какие-то слои, в терминологии глубинного обучения — мы их «замораживаем». В случае с предобученной моделью, мы должны «заморозить» нижние слои, в то время как верхние будут обновляться.  Архитектура нейросети: на нижних слоях модели хранятся базовые, общие знания, а на верхних — специфические закономерности. 

Наша «ягодная» модель уже знает очертания, характерные не только для клубники, но и в целом для любых ягод: это и есть нижние слои нейросети. Они не будут обучаться на нашем маленьком наборе фотографий — эти слои остаются такими же, как и в предобученной модели. Обучение происходит только на верхних слоях, которые и выучат образ, силуэт, характерный для клубники, а также признаки, характерные для больной клубники.

![](https://sysblok.ru/wp-content/uploads/2023/03/image1.jpg)

_Наглядная иллюстрация заморозки слоев во время трансферного обучения_

## Что такое трансферное обучение?

Идея трансферного обучения состоит в том, чтобы взять модель, обученную одной задаче, и применить ее ко второй, аналогичной задаче. Тот факт, что модель уже имеет некоторые или все данные для второй обученной задачи, означает, что она может быть реализована намного быстрее. Это позволяет быстро оценивать производительность и настраивать модель, обеспечивая более быстрое развертывание в целом. 

Трансферное обучение становится все более популярным в области глубокого обучения благодаря огромному количеству вычислительных ресурсов и времени, необходимых для обучения моделей глубокого обучения в дополнение к большим и сложным наборам данных. Основным ограничением трансферного обучения является то, что особенности модели, изученные во время первой задачи, являются общими, а не специфичными для первой. На практике это означает, что модели, обученные распознавать определенные типы изображений, могут быть повторно использованы для распознавания других изображений, если общие характеристики изображений аналогичны. 
## Теория трансферного обучения

Использование трансферного обучения имеет несколько важных концепций. Чтобы понять реализацию, нам нужно рассмотреть, как выглядит предварительно обученная модель и как ее можно настроить для ваших нужд. 

Выбрать модель для трансферного обучения можно двумя способами. Можно создать модель с нуля для собственных нужд, сохранить параметры и структуру модели, а затем повторно использовать ее позже. 

Второй способ реализовать трансферное обучение – просто взять уже существующую модель и повторно использовать ее, настраивая при этом ее параметры и гиперпараметры. В этом случае мы будем использовать предварительно обученную модель и модифицировать ее. После того, как вы решили, какой подход вы хотите использовать, выберите модель (если вы используете предварительно обученную модель). 
В PyTorch можно использовать множество предварительно обученных моделей. Некоторые из CNN включают: 

1. AlexNet; 
2. CaffeResNet; 
3. Inception; 
4. Серия ResNet; 
5. Серия VGG. 


Эти предварительно обученные модели доступны через API PyTorch, и после получения инструкций PyTorch загрузит их спецификации на ваш компьютер. Конкретная модель, которую мы собираемся использовать, – это ResNet34, часть серии Resnet. 

Модель Resnet была разработана и обучена на наборе данных ImageNet, а также на наборе данных CIFAR-10. Таким образом, он оптимизирован для задач визуального распознавания и показал заметное улучшение по сравнению с серией VGG, поэтому мы будем его использовать. 

Однако существуют и другие предварительно обученные модели, и вы можете поэкспериментировать с ними, чтобы увидеть, как они сравниваются. 

Как объясняется в документации PyTorch по трансферному обучению, существует два основных способа использования: точная настройка или использование CNN в качестве средства извлечения фиксированных функций. 

При тонкой настройке CNN вы используете параметры, которые имеет предварительно обученная сеть, вместо их случайной инициализации, а затем тренируетесь как обычно. Напротив, подход экстрактора признаков означает, что вы будете поддерживать все параметры CNN, за исключением тех, которые находятся на последних нескольких уровнях, которые будут инициализированы случайным образом и обучены как обычно. 

Точная настройка модели важна, потому что, хотя модель была предварительно обучена другой (хотя, надеюсь, схожей) задаче. Плотно связанных параметров, с которыми поставляется предварительно обученная модель, вероятно, будет недостаточно, поэтому вы, вероятно, захотите переобучить последние несколько слоев сети. 

Напротив, поскольку первые несколько слоев сети – это просто слои выделения признаков, и они будут работать аналогично с похожими изображениями, их можно оставить как есть. Следовательно, если набор данных небольшой и похожий, единственное обучение, которое необходимо выполнить, – это обучить нескольких последних слоев. Чем больше и сложнее будет набор данных, тем больше потребуется переобучения модели. 

Помните, что трансферное обучение работает лучше всего, когда набор данных, который вы используете, меньше, чем исходная предварительно обученная модель, и похож на изображения, подаваемые в предварительно обученную модель. 

Работа с моделями обучения передачи в Pytorch означает выбор слоев, которые нужно заморозить и разморозить. Замораживание модели означает указание PyTorch сохранить параметры в указанных вами слоях. Размораживание модели означает сообщение PyTorch о том, что вы хотите, чтобы указанные вами слои были доступны для обучения. 

После того, как вы завершили обучение выбранных вами слоев предварительно обученной модели, вы, вероятно, захотите сохранить параметры для использования в будущем. Несмотря на то, что использование предварительно обученных моделей происходит быстрее, чем обучение модели с нуля, обучение по-прежнему требует времени, поэтому вы захотите скопировать лучшие параметры модели.  
Точность (accuracy) классификации СНС изображений из тестовой выборки после классического обучения

|   |   |   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|---|---|
||MNIST|CIFAR-10|CIFAR-100|Fashion MNIST|STL-10|ASL Alphabet|Chest X-RAY Images|10 Monkey Species|
|AlexNet|0,9534|0,9497|0,6350|0,8043|0,9366|0,9497|0,8562|0,9405|
|VGG 16|0,9687|0,9251|0,5913|0,7513|0,9436|0,8067|0,7949|0,9428|
|VGG 19|0,9686|0,9172|0,6199|0,7664|0,9487|0,8705|0,7865|0,9488|

Таблица 2

Точность (accuracy) классификации СНС изображений из тестовой выборки после обучения с помощью transfer learning

|   |   |   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|---|---|
||MNIST|CIFAR-10|CIFAR-100|Fashion MNIST|STL-10|ASL Alphabet|Chest X-RAY Images|10 Monkey Species|
|AlexNet|0,9479|0,8381|0,6206|0,8709|0,9016|0,9643|0,9016|0,9540|
|VGG 16|0,9230|0,8423|0,6240|0,8627|0,9528|0,8214|0,8188|0,9717|
|VGG 19|0,9180|0,8438|0,6300|0,8619|0,9520|0,9301|0,7938|0,9817|
|ResNet 50|0,9234|0,8180|0,6176|0,8463|0,9388|0,9643|0,8438|0,9960|
|ResNet 101|0,9148|0,8024|0,5908|0,8483|0,9381|0,9643|0,8438|0,9831|
|ResNet 152|0,9171|0,8032|0,5896|0,8495|0,9508|0,9485|0,8422|0,9931|
|DenseNet 121|0,9374|0,8257|0,6115|0,8509|0,9606|0,9716|0,8094|0,9896|
|DenseNet 161|0,946|0,8837|0,6723|0,8727|0,9644|0,9878|0,8797|0,9966|
|DenseNet 169|0,9483|0,8545|0,6946|0,8666|0,9671|0,9286|0,8391|0,9930|

Критерием останова в процессе обучения является число эпох обучения. На такой процесс обучения было затрачено примерно от 5 до 8 ч.

В табл. 2 приведены показатели точности (accuracy) классификации изображений предобученными нейронными сетями после обучения с использованием методов transfer learning. Критерием останова в процессе обучения является число эпох обучения. На такой процесс обучения было затрачено от 3 мин до 4 ч (для нейросетей с очень большим числом слоев). Стоит отметить, что эти показатели могут быть выше, но для этого необходим другой критерий останова, а также требуется больше времени и более мощные компьютерные ресурсы.

Исходя из результатов, приведенных в табл. 1–2, можно сказать, что применение в процессе обучения методов transfer learning применимо для различных задач классификации, причем результаты классификации изображений из тестовых выборок довольно высокие в обоих вариантах обучения, однако на обычное обучение тратится больше времени. Более того, не всегда возможно полное обучение многослойных сверточных моделей сетей, так как зачастую для этого требуются мощные компьютерные ресурсы.
