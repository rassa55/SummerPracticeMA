Перекрестная проверка (кросс-валидация или скользящий контроль) — это статистический метод, используемый для оценки модели машинного обучения на независимых данных.

Проверка обычно используется в прикладном машинном обучении для сравнения и выбора модели для данной проблемы прогнозного моделирования, потому что она проста для понимания, проста в реализации и приводит к оценке качесва, которые обычно имеют более низкую предвзятость, чем другие методы.

В этом уроке мы рассмотрим процедуру кросс-валидации k-fold для оценки качества моделей машинного обучения.

После завершения этого урока, вы будете знать:

- Как использовать k-fold кросс-валидацию для оценки квалификации модели на новых данных.
- общие тактики, которые можно использовать, чтобы выбрать значение k для набора данных.
- широко используемые вариации на перекрестной проверки, такие как стратифицированы (stratified) и repeated, которые доступны в библиотеки scikit-learn.

Урок состоит из 5 частей:

- k-Fold Кросс-Валидация
- Конфигурация k
- Рабочий пример
- Работы с API кросс-валидации
- Вариации на кросс-валидации

## k-Fold кросс-Валидация

  
Перекрестная проверка — это процедура повторной выборки, используемая для оценки моделей машинного обучения на ограниченной выборке данных.

Процедура имеет один параметр, называемый k, который относится к числу групп, на которые должна быть разделена данная выборка данных. Таким образом, процедура часто называется перекрестной проверкой (кросс-валидацией) k-fold. При выборе определенного значения для k, оно может быть использовано вместо k в ссылке на модель, например, при k=10,  становится 10-кратной перекрестной проверкой.

Перекрестная проверка в основном используется в прикладном машинном обучении для оценки квалификации модели машинного обучения на не используемых данных. То есть использовать ограниченную выборку (test sample) для оценки того, как модель будет работать в целом при использовании ее при прогнозирования на данных, не используемых во время обучения модели.

Это популярный метод, потому что он прост для понимания и потому, что это обычно приводит к менее предвзятой или менее оптимистичной оценки качества модели, чем другие методы, такие как обучение / тест .

Общая процедура заключается в следующем:

1. Перемешайте датасет случайным образом

2. Разделите датасет на k-групп

3. Для каждой уникальной выборки:

- Возьмите группу в качестве тестирования датасета
- Возьмите остальные группы в качестве выборки учебных данных
- Приготовьте модель на обучаемых выборках и оцените ее на тестовой выборке
- Сохраняйте оценку модели и отбросьте модель

4. Обобщите параметры качества модели с помощью выборки оценки моделей

Важно отметить, что каждое наблюдение в выборке данных назначается отдельной группе и остается в этой группе в течение всего срока действия процедуры. Это означает, что каждому образцу предоставляется возможность использоваться в наборе 1 раз и использоваться для обучения модели k-1 раз.

Отдельно стоит подчеркнуть, чтобы любая подготовка данных до подбора модели происходила на выборке учебных данных, заданных кросс-валидацией в цикле, а не на более широком наборе данных. Это также относится к любой настройке гиперпараметров. Невыполнение этих операций в цикле может привести к утечке данных и оптимистичной оценке качества модели.

Результаты кросс-валидации k-fold часто суммируются со средним итогом качества модели. Также хорошей практикой является включение показателя дисперсии оценок качества, таких как стандартное отклонение или стандартная ошибка.

## Конфигурация параметра k

  
Значение k должно быть тщательно выбрано для выборки данных.

Плохо выбранное значение для k может привести к неправильному представлению о качестве модели, например, к оценке с высокой дисперсией (которая может сильно измениться на основе данных, используемых в соответствии с моделью), или к высокой предвзятости (например, переоценка качества модели).

Существуют три подхода для выбора значения параметра k:

- **Представитель**: Значение для k выбрано таким образом, что каждая группа подборка/тестовая группа данных достаточно велика, чтобы быть статистически репрезентативной для более широкого набора данных.
- **k=10**: Значение для k фиксируется до 10. Данное число было найдено в ходе экспериментов и обычно приводит к оценке качества модели с низкой предвзятостью небольшую дисперсии.
- **k = n**: Значение для k фиксируется на n, где n является размером набора данных, чтобы дать каждому тестовой группе возможность быть использованной в наборе данных.

k выбирают обычно 5 или 10, но нет формального правила. По мере того как k становится больше, разница в размере между тестовой выборкой и подмножествами resampling становится мала. По мере уменьшения этой разницы предвзятость к технике становится меньше.

Если выбрано значение для k, которое не делит датасет равномерно, то одна группа будет содержать оставшуюся часть примеров. Предпочтительно разделить исходный датасет на группы k с одинаковым количеством данных, так что выборка оценки качества моделей была эквивалентна.

## Рабочий пример

  
Чтобы сделать процедуру перекрестной проверки конкретной, давайте посмотрим на пример спроработанных.

Представьте, что у нас есть выборка данных с 6 наблюдениями:

```python
[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
```

Первым шагом является выбор значения для k для определения количества выборок, используемых для разделения данных. Здесь мы будем использовать значение k=3. Это означает, что мы будем тасовать данные, а затем разделить данные на 3 группы. Поскольку у нас есть 6 наблюдений, каждая группа будет иметь равно 2 наблюдения.

Например:

```python
subset1: [0,5, 0,2]
subset2: [0,1, 0,3]
subset3: [0,4, 0,6]
```

Затем мы можем использовать образец, например, для оценки качества алгоритма машинного обучения.

Три модели обучаются и оцениваются с каждой раз дается шанс быть протянутой набор испытаний.

Например:

- Модель1: обучаем на subset1 и subset2, тестируем на subset3
- Модель2: обучаем на subset2 и subset3, тестируем на subset1
- Модель3: обучаем на subset1 и subset3, тестируем на subset2

После того, как они модели оцениваются, затем отбрасываются поскольку они послужили своей цели.

Оценки качества собираются для каждой модели и суммируются для использования.

## API кросс-валидации  

Мы не должны осуществлять k-fold проверки вручную. Библиотека scikit-learn предоставляет реализацию, которая поможет разделить датасет .  
  
Метод KFold() из библиотеки scikit-learn может быть использован. Он принимает в качестве аргументов количество выборок на которое надо разбить датасет, следует ли перетасовывать датасет, и числовую затравку для псевдослучайного генератора чисел, используемого до перетасовки датасета.  
  
Например, мы можем создать экземпляр, который разделяет набор данных на 3 выборки, перетасует их до разделения и использует значение 1 для генератора псевдослучайных чисел.

```python
kfold = KFold(3, True, 1)
```

Функция split() может быть вызвана на классе, в котором в качестве аргумента приводится выборка данных. При повторном вызове, split() будет возвращать выборки данных на которых идет обучение и тестовый выборка. В частности, возвращаются массивы, содержащие индексы, в исходный датасет, в  можно указываются ссылки как на обучаемую выборку, так и на тестовую выборку на каждой итерации.

Например, мы можем перечислить разделения индексов для выборки данных с помощью созданного экземпляра KFold следующим образом:  
  
Перечисление выборок

```python
#перечисление выборок датасета
for train, test in kfold.split(data):
	print('train: %s, test: %s' % (train, test))
```

Мы можем связать все это вместе с нашим датасетом, используемым в примере из предыдущего раздела.

```python
# scikit-learn k-fold кросс-валидация
from numpy import arrayfrom
sklearn.model_selection import KFold

# датасет
data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])

# подготовьте кросс валидацию
kfold = KFold(3, True, 1)

# перечисление выборок датасета
for train, test in kfold.split(data):
	print('train: %s, test: %s' % (data[train], data[test]))
```

При выполнении примера выводятся конкретные наблюдения, выбранные для каждого поезда и тестового набора. Индексы используются непосредственно в исходном массиве данных для получения значений наблюдений.

```python
train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]
train: [0.2 0.3 0.4 0.6], test: [0.1 0.5]
train: [0.1 0.2 0.3 0.5], test: [0.4 0.6]
```

Реализация перекрестной проверки k-fold в scikit-learn предоставляется в качестве компонентной операции в рамках более широких методов, таких как гиперпараметры модели поиска сетки и оценка модели на наборе данных.  
  
Тем не менее, класс KFold может быть использован непосредственно для того, чтобы разделить набор данных до моделирования, чтобы все модели использовали одни и те же разделения данных. Это особенно полезно, если вы работаете с очень большим датасетом. Использование одних и тех же разделений между алгоритмами может иметь преимущества для статистических тестов, которые вы можете выполнить на данных позже.